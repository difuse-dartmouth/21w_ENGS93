{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RX3: \n",
    "\n",
    "Everyone who has applied to graduate school knows the power of the GRE. GRE scores can literally determine a person’s future. For this reason, many aspiring graduate students make use of test preparation or coaching classes. In this exercise, we will explore the concepts of statistical hypothesis testing, specificity, and power using examples related to GRE scores and coaching effectiveness.\n",
    "\n",
    "[![](https://i.imgur.com/YjLCe20.gif)\n",
    "](https://)\n",
    "\n",
    "## Hypothesis Testing (i.e., the Original “Standardized Test”)\n",
    "\n",
    "There is considerable debate regarding the effectiveness of various coaching programs in improving GRE scores. Here, we make use of a recent review on the topic, focusing on the **verbal** (GRE-V) part of the test. Data indicate that changes in the test scores for the general population of students retaking the test without any special coaching are normally distributed with a mean $\\mu$ of about 15 points and a standard deviation \t$\\sigma$ of about 40 points (This is the mean and standard deviation of the change in the score between tests. The SD of the scores on a single test is higher.) Thus 15 points represent the average improvement due to the learning effect and the natural increase in the verbal ability that occurs over time. If a coaching program does not improve the test scores by more than 15 points on average, then we say that the coaching program has no effect.\n",
    "\n",
    "------\n",
    "\n",
    "<span class=\"girk\">**EXERCISE 1:**</span>\n",
    "\n",
    "The Educational Testing Service (ETS), who administers the GRE, decides to test the\n",
    "claims of various coaching companies by performing hypothesis tests using a survey of students\n",
    "who have participated in the various companies’ coaching programs. The organization sets up the\n",
    "hypotheses as:\n",
    "\n",
    "$H_{0}$ : $\\mu$ $\\leq$ 15 (coaching program is ineffective) vs.\n",
    "\n",
    "$H_{1}$ : $\\mu$ > 15 (coaching program is\n",
    "effective)\n",
    "\n",
    "\n",
    "They then survey 20 students from each coaching program and calculate the mean change $\\bar{X}$ in GRE-V scores and decide to reject $H_{0}$ if $\\bar{X}$ > 25. Calculate the Type I error probability\n",
    "$\\alpha$ of this decision rule. (Note that 1 – $\\alpha$ is often reported as the **specificity** of the test.)\n",
    "\n",
    "This can be done manually but it may be easier to use R code to perform the necessary arithmetic. Remember to save important values (like the sample mean or standard deviation) as **variables** so we can use them later. Note: R does not recognise  $\\mu$ characters, so name them accordingly, like \"mu\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Question 1\n",
    "n=20\n",
    "mu0=15\n",
    "se0=40/sqrt(n)\n",
    "xcrit=25\n",
    "1-pnorm((xcrit-mu0)/(se0)) #=0.1317762. This is alpha."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use R to create a plot of the null probability density function (pdf) of the sample mean using the command: \n",
    "```\n",
    "curve(e1, from=v2, to=v3, xlim=c(-20,60))\n",
    "```\n",
    "\n",
    "We will replace e1 with the function dnorm(x,v4,v5). v2 and v3 are the extremes of the x range for which the function e1 is plotted.\n",
    "\n",
    "Set v2 and v3 such that the function is plotted between +/- 4 standard deviations from the mean, assuming that the null hypothesis is true. Note that the function curve() recognizes x exclusively as a variable to be plotted, even if you have not previously created a variable x. \n",
    "\n",
    "v4 and v5 respectively is the mean and standard deviation of the random variable under consideration, which in this case is the sample mean, under the assumption that the null hypothesis is true. If you did Exercise 1 correctly, these values should be saved in variables and easy to access. \n",
    "\n",
    "<span class=\"girk\">**EXERCISE 2:**</span> Create a plot of the null probability density function (pdf) of the sample mean using the **curve()** command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "curve(dnorm(x,mu0,se0),from=mu0-4*se0, to=mu0+4*se0, xlim=c(-20,60))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"girk\">**EXERCISE 3:**</span> Now we will add some deatils to the plot.In order to add all of these featurs you will need to call curve() to plot your graph first. Use your answer from Exercise 2 as the first command, and then perform the following commands: \n",
    "\n",
    "Add a red vertical line at the critical value 25, using the command: \n",
    "```\n",
    "abline(v=25, col=\"red\") \n",
    "```\n",
    "\n",
    "You can shade your Type I error region using the commands (using your own variable names where necessary) :\n",
    "\n",
    "```\n",
    "x=seq(from=xcrit, to=mu0+4*se0, by=0.1)\n",
    "y=dnorm(x, mu0, se0)\n",
    "polygon(c(25,x), c(0,y), col=\"grey\")\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "curve(dnorm(x,mu0,se0),from=mu0-4*se0, to=mu0+4*se0, xlim=c(-20,60))\n",
    "abline(v=25, col=\"red\")\n",
    "x=seq(from=xcrit, to=mu0+4*se0, by=0.1)\n",
    "y=dnorm(x, mu0, se0)\n",
    "polygon(c(25,x), c(0,y), col=\"grey\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Power of a Test\n",
    "\n",
    "<span class=\"girk\">**EXERCISE 4:**</span> The ACE coaching company prints an advertisement claiming that its actual mean improvement on the GRE-V is **40 points**. Calculate the Type II error probability $\\beta$ for the ETS’s testing procedure, given this claim. (Note that 1 – $\\beta$ is often reported as the **power** of the test.)\n",
    "\n",
    "Like in Exercise 1, R can help with computation and let you store important values as variables for later use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu1=40\n",
    "se1=40/sqrt(n)\n",
    "pnorm((xcrit-mu1)/(se1)) #=0.04676626. This is beta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to add a plot (like we did in Exercise 2) of the alternate pdf  using the command \n",
    "```\n",
    "curve(e1, from=v2, to=v3, col=\"blue\") \n",
    "```\n",
    "replacing expression e1 with the function dnorm(x,v4,v5). \n",
    "\n",
    "Set v2 and v3 such that the function is plotted between +/- 4 standard deviations from the mean, assuming that the specific alternate hypothesis is true. v4 and v5 respectively is the mean and standard deviation of the random variable under consideration, which in this case is the sample mean, under the assumption that the specific alternate hypothesis is true.\n",
    "\n",
    "<span class=\"girk\">**EXERCISE 5:**</span> Create a plot of the alternate probability density function (pdf) using the **curve()** command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "curve(dnorm(x,mu1,se1),from=mu1-4*se1, to=mu1+4*se1,col = \"blue\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great if we can overlay the plots to compare them on the same graph. Luckily there's a really easy way to do this in R. \n",
    "\n",
    "To overlay the 2 pdfs, first call the curve() command you did in Exercise 2, as well as all the plot details from Exercise 3. \n",
    "\n",
    "For the next command, use your answer from Exercise 5 but add the argument 'add=TRUE'. This will add the plot to an already existing plot.\n",
    "\n",
    "While we are at it, lets add some details to our 2nd pdf. Shade your Type II error region using the commands seq, dnorm and polygon, as we did before with your variables for the 2nd pdf. \n",
    "\n",
    "Finally, add some labels. Do this with:\n",
    "```\n",
    "text(\"type I error\", x=34, y=0.005) \n",
    "text(\"type II error\", x=16, y=0.005)\n",
    "```\n",
    "\n",
    "\n",
    "<span class=\"girk\">**EXERCISE 6:**</span> Overlay the 2 pdfs, include both Type 1 and 2 error regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#First pdf from Exercise 2&3\n",
    "curve(dnorm(x,mu0,se0),from=mu0-4*se0, to=mu0+4*se0, xlim=c(-20,60))\n",
    "abline(v=25, col=\"red\")\n",
    "x=seq(from=xcrit, to=mu0+4*se0, by=0.1)\n",
    "y=dnorm(x, mu0, se0)\n",
    "polygon(c(25,x), c(0,y), col=\"grey\")\n",
    "\n",
    "\n",
    "#Now overlay 2nd pdf\n",
    "curve(dnorm(x,mu1,se1),from=mu1-4*se1, to=mu1+4*se1, col=\"blue\", add=TRUE)\n",
    "x=seq(from=mu1-4*se1, to=xcrit, by=0.01)\n",
    "y=dnorm(x,mu1,se1)\n",
    "polygon(c(x,25), c(y,0), col=\"lightgrey\")\n",
    "text(\"type I error\", x=34, y=0.005)\n",
    "text(\"type II error\", x=16, y=0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"girk\">**EXERCISE 7:**</span> How do the Type I and II error rates change with increasing sample size? To determine this, find your new $\\alpha$ and then repeat the plotting steps of Exercises 2-6. Compare the two figures and describe how the Type I and Type II error rates change with increasing sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n=30\n",
    "mu0=15\n",
    "se0=40/sqrt(n)\n",
    "xcrit=25\n",
    "1-pnorm((xcrit-mu0)/(se0)) #=0.08545176. This is alpha.\n",
    "curve(dnorm(x,mu0,se0),from=mu0-4*se0, to=mu0+4*se0, xlim=c(-20,60))\n",
    "abline(v=25, col=\"red\")\n",
    "x=seq(from=xcrit, to=mu0+4*se0, by=0.1)\n",
    "y=dnorm(x, mu0, se0)\n",
    "polygon(c(25,x), c(0,y), col=\"grey\")\n",
    "mu1=40\n",
    "se1=40/sqrt(n)\n",
    "curve(dnorm(x,mu1,se1),from=mu1-4*se1, to=mu1+4*se1, col=\"blue\", add=TRUE)\n",
    "x=seq(mu1-4*se1, 25)\n",
    "y=dnorm(x,mu1,se1)\n",
    "polygon(c(x,25), c(y,0), col=\"lightgrey\")\n",
    "text(\"type I error\", x=34, y=0.005)\n",
    "text(\"type II error\", x=16, y=0.005)\n",
    "#Comparing the new figure with the previous one, \n",
    "#we find that the grey region has shrunk in area.\n",
    "#So the Type I error rate has gone down from before.\n",
    "#Similarly, the light grey region has also shrunk in area.\n",
    "#So the Type II error rate has also gone down compared to before.\n",
    "#Thus both Type I and Type II error rates decreased with increased sample size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
